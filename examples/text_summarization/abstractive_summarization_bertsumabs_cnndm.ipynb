{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation.\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization using BertSumAbs on CNN/DailyMails Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to fine tune BERT for abstractive text summarization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "### Abstractive Summarization\n",
    "Abstractive summarization is the task of taking an input text and summarizing its content in a shorter output text. In contrast to extractive summarization, abstractive summarization doesn't take sentences directly from the input text, instead, rephrases the input text.\n",
    "\n",
    "### BertSumAbs\n",
    "\n",
    "BertSumAbs refers to an BERT-based abstractive summarization algorithm  in [Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345) with [published examples](https://github.com/nlpyang/PreSumm). It uses the pretrained BERT model as encoder and finetune both encoder and decoder on a specific labeled summarization dataset like [CNN/DM dataset](https://github.com/harvardnlp/sent-summary). \n",
    "\n",
    "The figure below shows the comparison of architecture of the original BERT model (left) and BERTSUM (right), which BertSumAbs is built upon. For BERTSUM, a input document is split into sentences, and [CLS] and [SEP] tokens are inserted before and after each sentence. This resulting sequence is followed by the summation of three kinds of embeddings for each token before feeding into the transformer layers. The positional embedding used in BertSumAbs enables input length of more than 512, which is the  maximum input length for BERT model. \n",
    "\n",
    "It should be noted that the architecture only shows the encoder part. For decoder, BertSumAbs also uses a transformer with multiple layers and random initialization. As pretrained weights are used in the encoder, there is a mismatch in encoder and decoder which may result in unstable finetuning. Therefore, in fine tuning, BertSumAbs uses seperate optimizers for encoder and decoder, each uses its own scheduling. In text generation, techniques like trigram blocking and beam search can be used to improve model accuracy.\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/BertForSummarization.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "It's recommended to run this notebook on GPU machines as it's very computationally intensive. Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of steps. If QUICK_RUN = False, the notebook takes about 5 hours to run on a VM with 4 16GB NVIDIA V100 GPUs. Finetuning costs around 1.5 hours and inferecing costs around 3.5 hour.  Better performance can be achieved by increasing the MAX_STEPS.\n",
    "\n",
    "* **ROUGE Evalation**: To run rouge evaluation, please refer to the section of compute_rouge_perl in [summarization_evaluation.ipynb](./summarization_evaluation.ipynb) for setup.\n",
    "\n",
    "* **Distributed Training**:\n",
    "Please note that the jupyter notebook only allows to use pytorch [DataParallel](https://pytorch.org/docs/master/nn.html#dataparallel). Faster speed and larger batch size can be achieved with pytorch [DistributedDataParallel](https://pytorch.org/docs/master/notes/ddp.html)(DDP). Script [abstractive_summarization_bertsum_cnndm_distributed_train.py](./abstractive_summarization_bertsum_cnndm_distributed_train.py) shows an example of how to use DDP.\n",
    "\n",
    "* **Mixed Precision Training**:\n",
    "Please note that by default this notebook doesn't use mixed precision training. Faster speed and larger batch size can be achieved when you set FP16 to True. Refer to  https://nvidia.github.io/apex and https://github.com/nvidia/apex) for details to use mixed precision training. Check the GPU model on your machine to see if it allows mixed precision training. Please also note that mixed precision inferencing is also enabled in the prediciton utility function. When you use mixed precision training and/or inferencing, the model performance can be slightly worse than the full precision mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "import torch\n",
    "\n",
    "nlp_path = os.path.abspath(\"../../\")\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.models.transformers.abstractive_summarization_bertsum import (\n",
    "    BertSumAbs,\n",
    "    BertSumAbsProcessor,\n",
    ")\n",
    "from utils_nlp.models.transformers.datasets import (\n",
    "    SummarizationDataset\n",
    ")\n",
    "\n",
    "from utils_nlp.dataset.cnndm import CNNDMSummarizationDataset\n",
    "from utils_nlp.eval import compute_rouge_python\n",
    "\n",
    "from utils_nlp.models.transformers.datasets import SummarizationDataset\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import scrapbook as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation examples and ~11K test examples. The length of the news articles is 781 tokens on average and the summaries are of 3.75 sentences and 56 tokens on average.\n",
    "\n",
    "The significant part of data preprocessing only involve splitting the input document into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data path used to save the downloaded data file\n",
    "DATA_PATH = TemporaryDirectory().name\n",
    "# The number of lines at the head of data file used for preprocessing. -1 means all the lines.\n",
    "TOP_N = 100\n",
    "if not QUICK_RUN:\n",
    "    TOP_N = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 489k/489k [01:04<00:00, 7.55kKB/s]\n"
    }
   ],
   "source": [
    "train_dataset, test_dataset = CNNDMSummarizationDataset(\n",
    "    top_n=TOP_N, local_cache_path=DATA_PATH, prepare_extractive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "100"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "100"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "# the cache path\n",
    "CACHE_PATH = TemporaryDirectory().name\n",
    "\n",
    "# model parameters\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_POS = 768\n",
    "MAX_SOURCE_SEQ_LENGTH = 640\n",
    "MAX_TARGET_SEQ_LENGTH = 140\n",
    "\n",
    "# mixed precision setting. To enable mixed precision training, follow instructions in SETUP.md.\n",
    "FP16 = False\n",
    "if FP16:\n",
    "    FP16_OPT_LEVEL = \"O2\"\n",
    "\n",
    "# fine-tuning parameters\n",
    "# batch size, unit is the number of tokens\n",
    "BATCH_SIZE_PER_GPU = 1\n",
    "\n",
    "\n",
    "# GPU used for training\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "if NUM_GPUS > 0:\n",
    "    BATCH_SIZE = NUM_GPUS * BATCH_SIZE_PER_GPU\n",
    "else:\n",
    "    BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE_BERT = 5e-4 / 2.0\n",
    "LEARNING_RATE_DEC = 0.05 / 2.0\n",
    "\n",
    "\n",
    "# How often the statistics reports show up in training, unit is step.\n",
    "REPORT_EVERY = 10\n",
    "SAVE_EVERY = 500\n",
    "\n",
    "# total number of steps for training\n",
    "MAX_STEPS = 1e3\n",
    "\n",
    "if not QUICK_RUN:\n",
    "    MAX_STEPS = 5e3\n",
    "\n",
    "WARMUP_STEPS_BERT = 2000\n",
    "WARMUP_STEPS_DEC = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading: 100%|██████████| 433/433 [00:00<00:00, 32.1kB/s]\nDownloading: 100%|██████████| 232k/232k [00:00<00:00, 556kB/s]\nDownloading: 100%|██████████| 440M/440M [00:22<00:00, 19.4MB/s]\n"
    }
   ],
   "source": [
    "# processor which contains the colloate function to load the preprocessed data\n",
    "processor = BertSumAbsProcessor(cache_dir=CACHE_PATH, max_src_len=MAX_SOURCE_SEQ_LENGTH, max_tgt_len=MAX_TARGET_SEQ_LENGTH)\n",
    "# summarizer\n",
    "summarizer = BertSumAbs(\n",
    "    processor, cache_dir=CACHE_PATH, max_pos_length=MAX_POS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "BATCH_SIZE_PER_GPU*NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]device is cpu\nIteration:   2%|▏         | 2/100 [03:20<2:10:16, 79.76s/it]"
    }
   ],
   "source": [
    "summarizer.fit(\n",
    "    train_dataset,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate_bert=LEARNING_RATE_BERT,\n",
    "    learning_rate_dec=LEARNING_RATE_DEC,\n",
    "    warmup_steps_bert=WARMUP_STEPS_BERT,\n",
    "    warmup_steps_dec=WARMUP_STEPS_DEC,\n",
    "    save_every=SAVE_EVERY,\n",
    "    report_every=REPORT_EVERY * 5,\n",
    "    fp16=FP16,\n",
    "    # checkpoint=\"saved checkpoint path\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/marc/git/nlp-recipes/examples/text_summarization/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'summarizer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea9200acda83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCACHE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bertsumabs.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'summarizer' is not defined"
     ]
    }
   ],
   "source": [
    "summarizer.save_model(MAX_STEPS, os.path.join(path, \"bertsumabs.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "To run rouge evaluation, please refer to the section of compute_rouge_perl in [summarization_evaluation.ipynb](summarization_evaluation.ipynb) for setup.\n",
    "For the settings in this notebook with QUICK_RUN=False, you should get ROUGE scores close to the following numbers: <br />\n",
    "``\n",
    "{'rouge-1': {'f': 0.34819639878321873,\n",
    "             'p': 0.39977932634737307,\n",
    "             'r': 0.34429079596863604},\n",
    " 'rouge-2': {'f': 0.13919271352557894,\n",
    "             'p': 0.16129965067780644,\n",
    "             'r': 0.1372938054050938},\n",
    " 'rouge-l': {'f': 0.2313282318854973,\n",
    "             'p': 0.26664667422849747,\n",
    "             'r': 0.22850294283399628}}\n",
    " ``\n",
    " \n",
    " Better performance can be achieved by increasing the MAX_STEPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bert_sum(text:str)->str:\n",
    "    test_dataset = SummarizationDataset(\n",
    "        None, source=[text], source_preprocessing=[tokenize.sent_tokenize],\n",
    "    )\n",
    "    generated_summaries = summarizer.predict(test_dataset, batch_size=1, num_gpus=NUM_GPUS)\n",
    "    return generated_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " def lead_sum(text:str) -> str:\n",
    "    first_three = text.split(\".\")[0:3]\n",
    "    return \".\".join(first_three)+\".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "%run '/home/marc/git/nlp-recipes/examples/text_summarization/mytranslate.py'\n",
    "%run '/home/marc/git/nlp-recipes/examples/text_summarization/benchmark.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = GoogleTranslator(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerbert_sum(text:str)->str:\n",
    "    eng = translator.to_english(text)\n",
    "    sum = bert_sum(eng)\n",
    "    return translator.to_german(sum)\n",
    "\n",
    "def gerlead_sum(text:str)->str:\n",
    "    eng = translator.to_english(text)\n",
    "    sum = lead_sum(eng)\n",
    "    return translator.to_german(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = Workbench(\"/home/marc/git/nlp-recipes/utils_nlp/dataset/.data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Herr  Präsident!  Liebe  Kolleginnen  und  Kollegen!  In dieser Krise zeigt sich eine gewisse Doppelgesichtigkeit: Auf  der  einen  Seite  beweisen  der  Sozialstaat  und  die Sozialversicherungen in diesen Monaten, dass sie selbst in  der  jetzigen  Extremsituation  mit  Milliardeneinsatz  in der Lage sind, in der Krise ein gewisses Maß an Sicherheit zu geben. Auf der anderen Seite zeigt das umwälzende  Ausmaß  der  Krise  auch,  dass  all  diese  Gegenmaßnahmen   die   Erschütterungen   durch   Arbeitslosigkeit, Existenzverlust  und  Bildungsmangel  nicht  vollständig auffangen können. Tiefe Gräben tun sich auf.'"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "bench.test_sample(lead_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[                    ] 2% (11/500) evaluating bert_score (on gerlead_sum)"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-202619cf8b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgerlead_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/git/nlp-recipes/examples/text_summarization/benchmark.py\u001b[0m in \u001b[0;36mbenchmark\u001b[0;34m(self, model, num_samples)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpopulate_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/nlp-recipes/examples/text_summarization/benchmark.py\u001b[0m in \u001b[0;36mpopulate_scores\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mpercent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[%-20s] %d%%\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" (\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\") evaluating \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" (on \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\")\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/nlp-recipes/examples/text_summarization/benchmark.py\u001b[0m in \u001b[0;36mbert_score\u001b[0;34m(hypothesis, reference)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mbert_scorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTScorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert-base-german-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbert_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_scorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/bert_score/scorer.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, cands, refs, verbose, batch_size, return_hash)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mall_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         ).cpu()\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/bert_score/utils.py\u001b[0m in \u001b[0;36mbert_cos_score_idf\u001b[0;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0msen_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         embs, masks, padded_idf = get_bert_embedding(\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0msen_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         )\n\u001b[1;32m    385\u001b[0m         \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/bert_score/utils.py\u001b[0m in \u001b[0;36mget_bert_embedding\u001b[0;34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             batch_embedding = bert_encode(\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_sens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             )\n\u001b[1;32m    268\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/bert_score/utils.py\u001b[0m in \u001b[0;36mbert_encode\u001b[0;34m(model, x, attention_mask, all_layers)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         )\n\u001b[1;32m    736\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 408\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.platformio/penv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bench.no_bertscore()\n",
    "bench.benchmark(gerlead_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[====================] 100% (500/500) evaluating rl_score (on gerlead_sum)"
    }
   ],
   "source": [
    "bench.no_bertscore()\n",
    "bench.populate_scores(\"gerlead_sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[====================] 100% (500/500) evaluating rl_score (on lead_sum)"
    }
   ],
   "source": [
    "bench.benchmark(lead_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "cumulative": {
          "enabled": false
         },
         "legendgroup": "bert_score",
         "name": "gerlead_sum (0.62)",
         "opacity": 0.5,
         "type": "histogram",
         "x": [
          0.5930119752883911,
          0.5760954022407532,
          0.6161722540855408,
          0.6775783896446228,
          0.597238302230835,
          0.6191642880439758,
          0.6392707824707031,
          0.7482268810272217,
          0.525449275970459,
          0.5884507298469543
         ],
         "xaxis": "x",
         "yaxis": "y"
        },
        {
         "cumulative": {
          "enabled": false
         },
         "legendgroup": "r1_score",
         "name": "gerlead_sum (0.16)",
         "opacity": 0.5,
         "type": "histogram",
         "x": [
          0.1624999950195314,
          0.09523809039052683,
          0.1578947321021854,
          0.16129031908428726,
          0.20588234856401397,
          0.24175823798092025,
          0.169811315783197,
          0.28571428071636823,
          0.06896551538644476,
          0.19354838235171706,
          0.23287670733252028,
          0.15189873079955143,
          0.27499999537812503,
          0.07142856644292127,
          0.1558441509461968,
          0.2272727224819216,
          0.26923076497041426,
          0.3157894686842106,
          0.18518518074074086,
          0.2857142825142857,
          0.0999999968000001,
          0.15238094787483006,
          0.10526315574022779,
          0.2056074720552014,
          0.08849557022163079,
          0.243478256505104,
          0.19999999663265305,
          0.21818181338181827,
          0.22448979091836746,
          0.18556700575619098,
          0.13559321545532912,
          0.1714285671909298,
          0.24657533861512482,
          0.05714285214693921,
          0.09836065090029586,
          0.27848100766543826,
          0.1834862347142497,
          0.1799999957220001,
          0.16091953523583052,
          0.231404954005874,
          0.1355932167193336,
          0.2465753374892101,
          0.15789473272853197,
          0.14285713877551035,
          0.1515151480027549,
          0.19444444043209885,
          0.21739129968809084,
          0,
          0.24096385068369872,
          0.22448979105164527,
          0.12499999522959201,
          0.13636363310950422,
          0.3736263689457795,
          0.2702702653287072,
          0.2028985459021215,
          0.12698412453514743,
          0.2884615339349113,
          0.1505376298300383,
          0.21739129966032616,
          0.10416666188368078,
          0.09999999642222235,
          0.16091953557933691,
          0.29090908593131315,
          0.1931818138849433,
          0.24096385050950803,
          0.02272726775051762,
          0.11999999580000016,
          0.30894308455020164,
          0.06451612728407914,
          0.32608695160680534,
          0.19354838397502605,
          0.24489795485839239,
          0.24242423788337933,
          0.09523809256739742,
          0.25316455214228495,
          0.12307691881656818,
          0.21052631172077566,
          0.05797101037597172,
          0.13559321638609606,
          0.11764705611159176,
          0.24691357526596563,
          0.14117646753218,
          0.20382165219197543,
          0.04705882019100369,
          0.16161615784103672,
          0.02469135567748841,
          0.21818181318347118,
          0.14705881854671296,
          0.17475727684795941,
          0.06382978250792248,
          0.13953487969713368,
          0.14457830828857615,
          0.0784313704728951,
          0.2272727230191116,
          0.18497109377125875,
          0.16666666170138905,
          0.18604650782044357,
          0.14117646558892752,
          0.042553189262109666,
          0.14285714070803834,
          0.222222217246335,
          0.25287355836702347,
          0.0465116248783128,
          0.2303664875030839,
          0.16666666168619804,
          0.2568807294032489,
          0.13333332844088908,
          0.17857142362882664,
          0.15999999504355572,
          0.23076922790680476,
          0.202247186062366,
          0.172413788252081,
          0.13513513095690297,
          0.28070175106186523,
          0.23188405331127926,
          0.15999999680000007,
          0.33333332835884355,
          0.17391303903381655,
          0.22222221723765442,
          0.1016949106693481,
          0.126315786029917,
          0.15094339162691361,
          0.3016759726662714,
          0.21705425885463625,
          0.13333333033888894,
          0.22499999552812505,
          0.15999999512800014,
          0.126984125260771,
          0.21052631103379513,
          0.11363635966942162,
          0.1686746939178402,
          0.13913043076597367,
          0.2656249950012208,
          0.16842104769861513,
          0.19753085921658295,
          0.156028363927368,
          0.10526315457063723,
          0.18965516759363862,
          0.07999999520000028,
          0.1285714236367349,
          0.23129251225878117,
          0.12658227446242604,
          0.09195401813713858,
          0.206185562903603,
          0.1714285665306124,
          0.28571428192491816,
          0.21705425875368078,
          0.15999999680000007,
          0.21212120955004593,
          0.17721518597019717,
          0.15789473262119125,
          0.19230768737426052,
          0.08695651868094949,
          0.20512820013149255,
          0.16216215982468957,
          0.27999999596800007,
          0.12987012517456586,
          0.22222221722222232,
          0.12631578549806108,
          0.1944444395408952,
          0.15094339168209342,
          0.31999999596800005,
          0.1052631529701449,
          0.20155038300823278,
          0.19047618609705227,
          0.2077922036026312,
          0.0799999961520002,
          0.19672130812147276,
          0.11111110809327854,
          0.07792207606004391,
          0.08421052151578974,
          0.19696969198806258,
          0.20588234891435991,
          0.11764705414840465,
          0.17543859175130824,
          0.16666666176870762,
          0.159999997888,
          0.11320754435030267,
          0.17910447280017833,
          0.18181817719008275,
          0.07619047266394574,
          0.10256409830374771,
          0.19117646564121984,
          0.22857142426122457,
          0.1666666619791668,
          0.21739130176748583,
          0.15189872994712397,
          0.07894736633310254,
          0.25287355955608404,
          0.13513513190284887,
          0.23809523350340142,
          0.2903225758025495,
          0.12121211786960524,
          0.11320754319508736,
          0.20512820052596986,
          0.10989010489554424,
          0.2716049333638166,
          0.24561403011234237,
          0.16666666191358037,
          0.07843136964244532,
          0.33766233270366003,
          0.15384614889875098,
          0.18181817729586788,
          0.10526315447060644,
          0.15909090430010345,
          0.1842105214854572,
          0.0689655122948874,
          0.19178081733158203,
          0.1647058780179932,
          0.14999999580000012,
          0.07407407209876549,
          0.4380952331319728,
          0.12345678666666676,
          0.24444443946666677,
          0.3595505573286201,
          0.30555555074459884,
          0.11764705403546732,
          0.23076922650887582,
          0.18749999507812512,
          0.1818181768431794,
          0.13513513075237416,
          0.2222222181405896,
          0.15624999570312512,
          0.15686274065359493,
          0.20689654674791927,
          0.17777777485432103,
          0.15217390838374306,
          0.13513513057706372,
          0.21568626950980405,
          0.055555551948302716,
          0.16129031821019782,
          0.10526315315481707,
          0.19999999508888905,
          0.22222221795458016,
          0.1967213065842517,
          0.1333333284222224,
          0.17999999524200014,
          0.07692307200526001,
          0.08955223487636463,
          0.18181817712261777,
          0.2272727226446282,
          0.08695651674858251,
          0.17599999501568012,
          0.20155038310197718,
          0.14432989214156675,
          0.14457830827115709,
          0.17142856675918378,
          0.16279069345592226,
          0.2424242381416438,
          0.3255813903515414,
          0.14814814397805223,
          0.1509433934069064,
          0.1971830951001786,
          0.16326530129112884,
          0.14285713922902502,
          0.38235293633218,
          0.04545454194214903,
          0.05042016538380072,
          0.22857142360816338,
          0.1199999950720002,
          0.15384614931952675,
          0.15999999504355572,
          0.12307691841893507,
          0.138888885138889,
          0.14184396663346932,
          0.24999999523925787,
          0.14545454093223154,
          0.12121211715539242,
          0.2954545404958678,
          0.17391303983931955,
          0.24657533904297244,
          0.15053762966354506,
          0.19999999598765442,
          0.24324323825146102,
          0.11363635894886383,
          0.18333332833888902,
          0.22857142360816338,
          0.20869564725897935,
          0.21999999551200008,
          0.18461538004260364,
          0.1730769181952664,
          0.21739129951854927,
          0.05882352625913127,
          0.0833333295833335,
          0.24719100641838163,
          0.15384614926220427,
          0.12820512560157796,
          0,
          0.22988505261989708,
          0.2758620642568371,
          0.18749999595703135,
          0.1333333293209878,
          0.16260162107211334,
          0.10526315560941835,
          0.12345678605700364,
          0.24242423764921955,
          0.048780483804613986,
          0.16666666213348777,
          0.13114753696318207,
          0.22018348125915338,
          0.19230768790680483,
          0.03252032032256006,
          0.16326530213036244,
          0.09999999555555576,
          0.19148935670212777,
          0.19999999545000013,
          0.2898550677000631,
          0.2926829218359608,
          0.17241378839476829,
          0.10909090660495874,
          0.09523809024943337,
          0.07407406929431519,
          0.07999999608888908,
          0.2909090860743802,
          0.08695651676538571,
          0.14285713992346943,
          0.28282827782879305,
          0.09433961835350678,
          0.14925372635330825,
          0.13888888649691358,
          0.19469026127966177,
          0.07594936343534708,
          0.055045866570154485,
          0.1388888839236113,
          0.05217391017013249,
          0.11627906771227693,
          0.1428571383902542,
          0.2985074582757853,
          0.05714285332244924,
          0.09090908765495878,
          0.1627906933666848,
          0.15533980083702534,
          0.1684210496930748,
          0.241379305350773,
          0.05479451581910343,
          0.022727269752066504,
          0.12048192359994207,
          0.17499999511250014,
          0.1249999950000002,
          0.03124999507812578,
          0.14705881863754341,
          0.104166662293837,
          0.07017543427516186,
          0.04411764205990541,
          0,
          0.21276595255771855,
          0.2121212078971534,
          0.08695651711825271,
          0.05633802483634218,
          0.14705881926038075,
          0.01980197739829468,
          0.18461537971124273,
          0,
          0.08064515779396476,
          0.07499999820000004,
          0.12121211753902673,
          0.01515151208103827,
          0.0425531866465475,
          0.1621621571658146,
          0.01724137440547107,
          0.15384614922352388,
          0.07766989793571527,
          0.08955223501893533,
          0.10810810372534715,
          0.08510638015391589,
          0.07692307195266304,
          0.042105258561773364,
          0.04411764284710248,
          0.12499999695312508,
          0.07920791786295472,
          0.05607476148135251,
          0.06493506010035455,
          0.17391303905330824,
          0.27692307321183435,
          0.23655913499364098,
          0.10909090529586792,
          0.11214952812996787,
          0.04545454071281041,
          0.063829782888185,
          0.1176470556708959,
          0.029197075547978818,
          0.039999996568000296,
          0.1666666618836807,
          0.15624999595703135,
          0.02409638290027609,
          0.20183485816345434,
          0.15151514754820947,
          0.07228915164465124,
          0.08108107621256422,
          0.18045112339193858,
          0.23684210029432146,
          0.06153845696568081,
          0.07499999580000023,
          0.036363631735537784,
          0.18181817733406805,
          0.19230768731097975,
          0.19298245121575883,
          0.1939393889704317,
          0.18965516849732472,
          0.2499999957031251,
          0.17910447382490538,
          0.11764705591695508,
          0.0606060556922767,
          0.21212120749311303,
          0.21874999507812512,
          0.2105263111357342,
          0.1592920305395882,
          0.23188405411468183,
          0.11764705383235667,
          0.2222222173510137,
          0.17460316975560608,
          0.25396825069286977,
          0.3423423377842708,
          0.022222218891358527,
          0.16470587740899667,
          0.09374999539550805,
          0.02105262780277076,
          0.10909090468760349,
          0.07207206812758726,
          0.07017543378270269,
          0.2638888840277778,
          0.04705881883460255,
          0.28571428145393074,
          0.02898550384373071,
          0.11267605173477505,
          0.024999996250000565,
          0.14141413754106733,
          0.07843136875048076,
          0.10204081224489812,
          0.22222221724011992,
          0.06153845653964538,
          0.02173912545605029,
          0.09876542825788766,
          0.23684210094182825,
          0.20588234804930808,
          0.06779660580293048,
          0.08163265032902967,
          0.12244897685964186,
          0.08108107769174595,
          0.37499999501953135,
          0.11538461045118363,
          0.1224489745939194,
          0.025316450793143076,
          0.19672130663800066,
          0.10909090437024814,
          0.07272726975206624,
          0.21621621123082552,
          0.2372881313530595,
          0.17647058334342575,
          0.21428570935657607,
          0.16949152239011783,
          0.22857142362630392,
          0,
          0.1791044726665183,
          0.23076922666420124,
          0.21568626954056144,
          0.13043477794896047,
          0.034482757486623106,
          0.0588235250346024,
          0.05714285215918411,
          0.18823528911833923,
          0.054054051336742286,
          0.19417475307380536,
          0.11235954589572043,
          0.13793102996432832,
          0.1999999950500001,
          0.1739130394875027,
          0.14814814370370383,
          0.1176470540733566,
          0.31034482318073725,
          0.1587301537918873,
          0.05970149043439526,
          0.13698629644586244,
          0.1212121172451792,
          0.14414413982631294,
          0.21428570969387764,
          0.027777774027778285,
          0.21476509640286479,
          0.020833330418837216,
          0,
          0.1999999954231406,
          0.03999999788800011,
          0.016260158331681998,
          0.08421052365650979,
          0.08695651707939533,
          0.08474575782821057,
          0.2539682493323256,
          0.0645161255359003,
          0.22222221793580252,
          0.23255813531638725,
          0.15384614920858003,
          0.09876542727023344,
          0.05042016455900031,
          0.23008849105489865,
          0.2857142812081633,
          0.13953488069226613,
          0.2413793056361475,
          0.19444444083719137,
          0.06315789106703623,
          0.22641508948380215
         ],
         "xaxis": "x2",
         "yaxis": "y2"
        },
        {
         "cumulative": {
          "enabled": false
         },
         "legendgroup": "r1_score",
         "name": "lead_sum (0.19)",
         "opacity": 0.5,
         "type": "histogram",
         "x": [
          0.16049382217268726,
          0.06349205864449521,
          0.1578947321021854,
          0.22222221793580252,
          0.24657533825483213,
          0.2619047579280046,
          0.18867924031149888,
          0.27999999500000006,
          0.030303027947658587,
          0.22641508938412258,
          0.18487394491914425,
          0.17721518649575393,
          0.28930817150112736,
          0.10344827089922734,
          0.20512820021038802,
          0.20689654691504833,
          0.25925925508916325,
          0.3037974633616408,
          0.17610062457339512,
          0.26470587908304505,
          0.09230768929704153,
          0.17307691861131666,
          0.14999999711250006,
          0.21359222835328506,
          0.05555555225694465,
          0.24561403070329338,
          0.19444444114583334,
          0.31192660069017764,
          0.16494844860877897,
          0.14141413690439766,
          0.14035087237919375,
          0.16071428163265317,
          0.2769230727857988,
          0.036363632092562485,
          0.13953487885343446,
          0.34999999501250006,
          0.17857142482142865,
          0.17475727734565003,
          0.18390804098295693,
          0.19999999500000015,
          0.14285713910714296,
          0.26751591856708185,
          0.14634145948839988,
          0.14285713877551035,
          0.13333332958333344,
          0.25714285306122453,
          0.1960784269281047,
          0.04545454220041345,
          0.20895521889062166,
          0.20408162778633912,
          0.10619468551022028,
          0.15384615067262414,
          0.3835616388590731,
          0.24324323830168015,
          0.18181817693296617,
          0.15151514915977962,
          0.2733812911257182,
          0.19354838251821027,
          0.25210083546359724,
          0.10204081157850917,
          0.06557376695511978,
          0.19565216937618155,
          0.25301204321889975,
          0.23595505182615834,
          0.23170731218024995,
          0.06666666167654359,
          0.15841583732967368,
          0.29032257575052034,
          0.08823529250865055,
          0.32608695160680534,
          0.19230768875739648,
          0.2545454504975207,
          0.2812499953955079,
          0.13333333102222228,
          0.37499999520000005,
          0.12499999570312517,
          0.2413793053686089,
          0.05714285306122478,
          0.11320754295478834,
          0.11764705611159176,
          0.2650602359703877,
          0.1627906945024338,
          0.20125785768917373,
          0.04761904425170092,
          0.224999995703125,
          0.09756097168352187,
          0.1978021930684701,
          0.14084506547113684,
          0.17821781704146664,
          0.10101009638200205,
          0.14457830914211073,
          0.17977527589950779,
          0.11999999788800003,
          0.24390243460737665,
          0.1917808184950273,
          0.10958903611934719,
          0.13953487991346683,
          0.16470587735363335,
          0.0869565194706995,
          0.1999999971591837,
          0.31999999502592,
          0.2142857095153062,
          0.09090908793388439,
          0.2650602360865148,
          0.15555555055555573,
          0.27586206456004764,
          0.11428570932244919,
          0.19999999531250012,
          0.21621621124908705,
          0.22641509152011394,
          0.15053762942305482,
          0.13793102963139137,
          0.2745097990157632,
          0.23076922579881665,
          0.24285713822551025,
          0.23809523350340142,
          0.34883720431314225,
          0.15873015409423044,
          0.2727272677537191,
          0.13333332878333348,
          0.10309278011265821,
          0.14285713837372463,
          0.2794117604368513,
          0.2499999950021702,
          0.09836065278151043,
          0.1951219468024986,
          0.18947367927756245,
          0.19999999745000002,
          0.22680411899245415,
          0.08888888497777794,
          0.16470587750865068,
          0.1758241712643402,
          0.188679240539338,
          0.14583332838758697,
          0.21428570933106586,
          0.13483145620502476,
          0.1449275326696073,
          0.1599999957220001,
          0.08108107631482862,
          0.14285713792244917,
          0.24999999516966767,
          0.09999999601250016,
          0.12903225331945908,
          0.19999999596800008,
          0.18867924039693854,
          0.315789469646045,
          0.19548871704222975,
          0.23999999520000007,
          0.19999999755102044,
          0.18918918512417832,
          0.24561403027393053,
          0.2264150894268424,
          0.12658227573145336,
          0.15384614884944134,
          0.1578947345567867,
          0.3435582777372125,
          0.12499999537812517,
          0.3458646566679858,
          0.12371133628228304,
          0.11940298010247291,
          0.1523809478167802,
          0.3333333292013889,
          0.06976743699296951,
          0.16774193130988563,
          0.19354838245808778,
          0.26229507726955126,
          0.06451612416233128,
          0.22222221894683802,
          0.18181817759412314,
          0.15789473351800565,
          0.11764705389273376,
          0.16260162111177226,
          0.2686567123546448,
          0.13592232539164875,
          0.1454545406545456,
          0.14285713786122467,
          0.15094339421858313,
          0.1052631542797785,
          0.1739130387145559,
          0.21212120749311303,
          0.14634145927424164,
          0.15189872994712397,
          0.19696969199839315,
          0.15873015418493333,
          0.1568627405305653,
          0.1702127634223631,
          0.202531641339529,
          0.07499999800312505,
          0.31999999596800005,
          0.12195121652290311,
          0.21818181334710754,
          0.33333332844583335,
          0.09090908756657497,
          0.1960784273029605,
          0.22222221769852166,
          0.11904761411848094,
          0.31707316580606787,
          0.18556700564140727,
          0.16216215745799867,
          0.07999999704800012,
          0.42253520626859753,
          0.14035087245306266,
          0.17543859204678372,
          0.12030074745208903,
          0.13636363157283074,
          0.17721518510495124,
          0.09836065084654688,
          0.18918918463111772,
          0.22222221802222225,
          0.2962962913580247,
          0.10958903834865835,
          0.459999995008,
          0.17499999651250006,
          0.2150537584876865,
          0.39534883266630605,
          0.21621621144996359,
          0.08823528932958502,
          0.21052631175130812,
          0.18181817685950424,
          0.21052631101108046,
          0.14084506594723284,
          0.17647058434256063,
          0.2424242382001837,
          0.11320754281238891,
          0.17857142357780628,
          0.14492753418189458,
          0.17283950130163098,
          0.15151514674012873,
          0.270270265303141,
          0.05882352566176495,
          0.17391303936147878,
          0.1016949105774205,
          0.20155038278468854,
          0.23255813541103307,
          0.1562499951757814,
          0.19354838222684714,
          0.1927710793554944,
          0.13157894249307495,
          0.07499999651250017,
          0.20779220309664373,
          0.31818181355371905,
          0.1558441509461968,
          0.15789473196675916,
          0.2045454495454547,
          0.23376622876707717,
          0.1481481431854902,
          0.1568627400999617,
          0.13953487950243387,
          0.3478260824648183,
          0.36363635864669425,
          0.17721518564332647,
          0.2127659543503848,
          0.27999999564800004,
          0.1666666618055557,
          0.17283950245389432,
          0.34210525850415513,
          0.13333332888888905,
          0.03361344269472517,
          0.23423422932554186,
          0.16363635883636377,
          0.1454545410512398,
          0.10344827101070177,
          0.12903225331945908,
          0.1686746953984614,
          0.2127659524490721,
          0.2941176424091696,
          0.13461538035502973,
          0.13861385737868848,
          0.27272726776859507,
          0.17391303983931955,
          0.273972599316945,
          0.25490195621683975,
          0.1599999955555557,
          0.31578946868507624,
          0.13793102977407862,
          0.18644067296753822,
          0.28571428075102046,
          0.19130434291115325,
          0.22222221771247844,
          0.17910447310314112,
          0.2037036988751716,
          0.1999999951085874,
          0.09803921253364101,
          0.10526315357340738,
          0.2666666618666667,
          0.13999999557800014,
          0.10666666397866674,
          0.3174603124918116,
          0.22988505261989708,
          0.27272726798553726,
          0.18181817823579025,
          0.15533980116881907,
          0.14062499501953143,
          0.13114753824240802,
          0.1999999955877552,
          0.2524271796022246,
          0.20512820100518675,
          0.16438355714017652,
          0.16129031860041634,
          0.2807017494413666,
          0.22499999507812513,
          0.13913042982835558,
          0.1782178178649153,
          0.14035087263773482,
          0.14893616521276612,
          0.22950819220639618,
          0.3013698583524114,
          0.3720930185085031,
          0.17857142380102056,
          0.1071428546938776,
          0.06896551233650451,
          0.07142856656746066,
          0.11940298088661186,
          0.23853210527733368,
          0.11267605138662985,
          0.1454545424793389,
          0.3636363586368738,
          0.31858406666144573,
          0.0784313682429837,
          0.16216215982468957,
          0.20168066818727498,
          0.06896551382745426,
          0.11764705389273376,
          0.22222221725694455,
          0.11764705602711678,
          0.25531914558623814,
          0.13861385698657008,
          0.26086956087376606,
          0.11428571046530626,
          0.14925372635330825,
          0.3809523765901361,
          0.17475727656518064,
          0.17073170403033916,
          0.19047618549841283,
          0.2028985459021215,
          0.16279069464575452,
          0.14705881896193787,
          0.29729729233016805,
          0.23529411266435996,
          0.3025210034206624,
          0.13114753599570028,
          0.2365591353405019,
          0.16666666246666675,
          0.3511450331822155,
          0.14457830863695761,
          0.27368420565096957,
          0.1904761861426053,
          0.05479451604428635,
          0.2016806684979875,
          0.4705882305709343,
          0.051282047873110026,
          0.1587301538825902,
          0.09876542814205168,
          0.18518518134430736,
          0.24390243588340277,
          0.2127659536441829,
          0.11864406445705268,
          0.23255813494621722,
          0.08695651774837238,
          0.10344827095719404,
          0.2769230719242604,
          0.29411764208958097,
          0.18749999609863285,
          0.21212120749311303,
          0.09523809215419511,
          0.11764705387158807,
          0.10666666171022246,
          0.1864406734314853,
          0.12244897659308628,
          0.11627906645754471,
          0.18867924039693854,
          0.258064511130333,
          0.2075471652118193,
          0.24999999591836736,
          0.3366336584256445,
          0.11111110727023334,
          0.18705035471248915,
          0.05128204636423453,
          0.14999999531250016,
          0.19607842821991545,
          0.1956521690288281,
          0.12499999646701399,
          0.08163264831320312,
          0.13953487885343446,
          0.15094339252403,
          0.22018348201666535,
          0.17499999651250006,
          0.13793102953626654,
          0.0869565167905905,
          0.24427480480391592,
          0.2531645519692358,
          0.15873015409423044,
          0.07407406913580279,
          0.2692307644970415,
          0.17177913667356706,
          0.21818181321285593,
          0.21238937559714943,
          0.21301774649347022,
          0.19298245234687603,
          0.3023255772714981,
          0.14492753251837862,
          0.14084506761356877,
          0.12121211629833711,
          0.1643835572302497,
          0.17948717488494426,
          0.2105263111357342,
          0.17543859161588196,
          0.25454545014214885,
          0.2095238045605443,
          0.21428570946712028,
          0.18897637308946633,
          0.2807017508648815,
          0.3773584859149164,
          0.10526315414473697,
          0.21428570933106586,
          0.23333332860555567,
          0.24137930549346026,
          0.15384614931952675,
          0.0999999969220001,
          0.19230768737426052,
          0.22377621890557006,
          0.12820512336620662,
          0.29787233624264375,
          0.17910447413677882,
          0.19354838268137364,
          0.17910447342392521,
          0.17647058443675517,
          0.17721518545745882,
          0.16949152043665627,
          0.2352941126756586,
          0.13114753599570028,
          0.18947367925540182,
          0.07142856767857161,
          0.2337662293810087,
          0.20512820012857053,
          0.16393442194033875,
          0.1538461512426036,
          0.12244897685964186,
          0.11320754295478834,
          0.3960395990197039,
          0.1818181769656159,
          0.1199999950000002,
          0.13513513036888256,
          0.1333333283950619,
          0.2641509386116056,
          0.09302325224445661,
          0.18421052131925222,
          0.33898304660729683,
          0.1176470539316611,
          0.26190475697562365,
          0.5306122399500208,
          0.19780219317956782,
          0.28235293623252594,
          0.31404958202581795,
          0.21276595310095073,
          0.1980197970042154,
          0.18691588351471755,
          0.06153845961656811,
          0.04651162290968145,
          0.258064511149844,
          0.18823528911833923,
          0.05263157628808878,
          0.180180176154533,
          0.21818181398181827,
          0.24999999513888896,
          0.21052631081662063,
          0.16901408058718514,
          0.14925372741367798,
          0.12499999500488301,
          0.31034482318073725,
          0.1538461488757398,
          0.0634920612748804,
          0.21621621130752386,
          0.17543859216989854,
          0.1964285671316965,
          0.2121212078971534,
          0.112676052553065,
          0.20689654760903692,
          0.1304347795959358,
          0.18556700564140727,
          0.19469026081603896,
          0.06666666486666671,
          0.2631578897368422,
          0.10752687901491507,
          0.20289854701113216,
          0.08749999570312521,
          0.26666666193888894,
          0.22222221838134434,
          0.2340425490086012,
          0.2666666617555556,
          0.19230768767011847,
          0.1282051233234716,
          0.2318840535265701,
          0.21052631123422602,
          0.22222221777777784,
          0.13333332942222234,
          0.19047618593096505,
          0.19444444083719137,
          0.14736841738282558,
          0.19753085923487287
         ],
         "xaxis": "x2",
         "yaxis": "y2"
        },
        {
         "cumulative": {
          "enabled": false
         },
         "legendgroup": "rl_score",
         "name": "gerlead_sum (0.15)",
         "opacity": 0.5,
         "type": "histogram",
         "x": [
          0.1363636313923326,
          0.03389830025854708,
          0.12371133535976209,
          0.1818181782082645,
          0.19354838272632682,
          0.18421052231648208,
          0.13043477761814765,
          0.2999999950500001,
          0.08571428347346943,
          0.18181817701818195,
          0.16666666167222238,
          0.1818181779935722,
          0.23728813080005756,
          0.06593406093467011,
          0.15151514662993587,
          0.18421052144044334,
          0.31818181355371905,
          0.2999999950000001,
          0.1641790999231456,
          0.2758620654875149,
          0.11111110786694109,
          0.0952380905215422,
          0.13043478002835543,
          0.21505375864955495,
          0.07843136755094227,
          0.21505375914903463,
          0.15624999658203134,
          0.15730336592854452,
          0.1978021928028017,
          0.18421052173476465,
          0.1632653011411913,
          0.16279069313142253,
          0.19047618653565138,
          0.06451612403746138,
          0.10714285230229613,
          0.2777777727932099,
          0.19565216985822315,
          0.1904761859552155,
          0.15384614884944134,
          0.17475727684795941,
          0.15686274149942336,
          0.19999999501250013,
          0.18181817737373748,
          0.17857142434311235,
          0.13333332958333344,
          0.19672130718624034,
          0.18604650684694443,
          0,
          0.18666666192355566,
          0.1627906927717687,
          0.10752687692912496,
          0.15789473336911364,
          0.2739725981534998,
          0.2999999950000001,
          0.1935483822840792,
          0.1509433934069064,
          0.17977527650549185,
          0.10958903677988382,
          0.20979020496845827,
          0.11494252384727198,
          0.11538461163461551,
          0.1866666620586668,
          0.22727272227272738,
          0.1594202853182106,
          0.1503759349313134,
          0.023255808977826916,
          0.11235954616841326,
          0.31775700444405625,
          0.07407407209876549,
          0.25641025152859964,
          0.14814814469135812,
          0.25641025173898757,
          0.15384614974112437,
          0.11320754410822365,
          0.18461537975858,
          0.1379310303388824,
          0.2051282008678502,
          0.07017543404124375,
          0.15384614958579892,
          0.13333333033888894,
          0.2571428521428572,
          0.11267605317198978,
          0.14545454132892574,
          0.0579710107792483,
          0.17499999570312508,
          0.03225806159729475,
          0.20224718601186734,
          0.10909090409256221,
          0.11235954584017191,
          0.07058823054394496,
          0.17391303913463568,
          0.13333332837688905,
          0.0833333311458334,
          0.2399999955555556,
          0.11764705430038944,
          0.15873015373141866,
          0.19999999601250007,
          0.15999999500800016,
          0.04878048530636538,
          0.13157894470914133,
          0.14953270528081072,
          0.2749999951531251,
          0.05263157562326891,
          0.20895521913120974,
          0.20512820015779104,
          0.22222221763950628,
          0.08823528927335667,
          0.1568627403306422,
          0.08823528915657468,
          0.26086956206994333,
          0.19753085923487287,
          0.1886792404271984,
          0.1428571385469389,
          0.279999996352,
          0.2616822381832475,
          0.16129031908428726,
          0.37837837341124914,
          0.13559321585751236,
          0.1684210476454295,
          0.1199999953920002,
          0.10666666275555571,
          0.17021276123132653,
          0.22058823030384958,
          0.18691588304306067,
          0.14545454225454552,
          0.23529411307958487,
          0.1333333284222224,
          0.16326530397334446,
          0.19277107954710418,
          0.08695651774837238,
          0.1538461488757398,
          0.1428571384948981,
          0.27272726772892564,
          0.14999999502812517,
          0.1971830936163461,
          0.1441441393977763,
          0.12499999625000012,
          0.17999999507200018,
          0.09230768740355055,
          0.15384614892833676,
          0.17857142370057413,
          0.14705881926038075,
          0.029850741358878063,
          0.21951219070493763,
          0.13793102951512765,
          0.30188678856532575,
          0.1714285665306124,
          0.17391304007561445,
          0.2068965488703924,
          0.1791044736823347,
          0.14084506604245203,
          0.15999999507200013,
          0.09999999660555567,
          0.23529411266435996,
          0.19354838439125913,
          0.19354838290972953,
          0.13888888413580264,
          0.19230768732433445,
          0.11627906574364534,
          0.09523809045099545,
          0.10989010515638228,
          0.2727272683884298,
          0.08333332847222251,
          0.13084111686260827,
          0.1445783087066339,
          0.2187499954882813,
          0.08333332938368075,
          0.23529411384851984,
          0.13043477920604923,
          0.08571428368979596,
          0.09756097075550292,
          0.14545454045454562,
          0.19999999580000008,
          0.13953487899134684,
          0.1960784264667437,
          0.18421052140235472,
          0.18181817946280993,
          0.12499999695312508,
          0.17241378819857325,
          0.124999995395508,
          0.06741572650422949,
          0.08823528955017325,
          0.12389380031952406,
          0.21212120778236923,
          0.1739130387145559,
          0.22727272459710743,
          0.18181817727731878,
          0.08571428347346943,
          0.27397259876149377,
          0.12903225456815828,
          0.20833332843967028,
          0.28846153352810655,
          0.13114753744692295,
          0.08988763651054177,
          0.20895521913120974,
          0.07594936212786445,
          0.1967213065090031,
          0.19417475231972864,
          0.12499999517578143,
          0.09302325250405635,
          0.32876711833364614,
          0.17910447263978627,
          0.1999999952880001,
          0.08163264946897142,
          0.14999999520000015,
          0.15151514674012873,
          0.08163264807996698,
          0.19047618562862195,
          0.14705881896193787,
          0.17142856734693887,
          0.09523809278911571,
          0.42696628716576196,
          0.14285713933061234,
          0.2597402547478496,
          0.256410251965812,
          0.23188405314849833,
          0.09230768757396474,
          0.2499999955555556,
          0.19354838222684714,
          0.16470587735916972,
          0.12121211676767694,
          0.2456140307663897,
          0.17241378858501796,
          0.12765956997736547,
          0.18518518021262018,
          0.19047618739229027,
          0.14999999537812514,
          0.12121211667125821,
          0.13636363140495886,
          0.06666666308888908,
          0.15094339162691361,
          0.11538461065088776,
          0.16822429411826373,
          0.22950819220639618,
          0.19417475241775864,
          0.14285713788265325,
          0.16091953537851778,
          0.0983606507712983,
          0.10526315401662063,
          0.17391303860533516,
          0.24999999531250006,
          0.09677418855359028,
          0.155339800863418,
          0.15999999539200013,
          0.12987012501264988,
          0.13698629637080145,
          0.19354838235171706,
          0.13157894304709156,
          0.23529411320261448,
          0.27027026527027037,
          0.18461537995739655,
          0.18604650831800976,
          0.22222221843285464,
          0.17021276106835687,
          0.13888888500385813,
          0.3157894687596184,
          0.052631575069252354,
          0.061855667080455035,
          0.1914893567383433,
          0.0714285665306126,
          0.1666666619791668,
          0.1791044726665183,
          0.1311475362859448,
          0.1403508730070792,
          0.12280701255001558,
          0.2641509386116056,
          0.14141413684317944,
          0.13513513075237416,
          0.2307692257823801,
          0.1951219472932779,
          0.24242423875114788,
          0.09638553737842961,
          0.16438355732782897,
          0.19999999502644641,
          0.05479451575905465,
          0.1904761854802722,
          0.21538461041420132,
          0.1599999950222224,
          0.19999999535555565,
          0.17543859175130824,
          0.14999999507812514,
          0.1923076874465813,
          0.07692307310979637,
          0.09836065171728046,
          0.23376622895597918,
          0.15217390838374306,
          0.12698412390022681,
          0,
          0.18918918432067217,
          0.23529411292387553,
          0.16393442206933634,
          0.14814814388050612,
          0.15929203048633425,
          0.12698412431342912,
          0.1111111068672841,
          0.2432432385390797,
          0.04301074817435589,
          0.16949152094225808,
          0.1379310303388824,
          0.15384614885158815,
          0.20833332888888897,
          0.04255318652557777,
          0.1379310303871054,
          0.11764705425605555,
          0.14117646558892752,
          0.21818181346115711,
          0.3333333287722908,
          0.27419354340790847,
          0.14814814348422511,
          0.08333333055555565,
          0.10714285214923494,
          0.08108107631482862,
          0.09677418956815832,
          0.19148935692847455,
          0.09677418855359028,
          0.15999999680000007,
          0.26506023599941947,
          0.1111111064236113,
          0.07407406910150927,
          0.16949152260844588,
          0.15909090452737618,
          0.10169491088767613,
          0.04347825589083233,
          0.15873015373141866,
          0.06593406263011731,
          0.1492537288037425,
          0.14814814370370383,
          0.29032257606659734,
          0.05714285332244924,
          0.09523809187074841,
          0.12499999531250018,
          0.09302325081395375,
          0.16438355837868274,
          0.21568626952710504,
          0.033333328533334025,
          0.031249996250000453,
          0.1388888844444446,
          0.18918918442293656,
          0.13793102950654001,
          0.03921568127643278,
          0.126984122086168,
          0.07792207336819051,
          0.07692307239644997,
          0.057692302721893914,
          0,
          0.1714285664938777,
          0.22641509028123896,
          0.0952380905215422,
          0.06060605663911872,
          0.15999999520000013,
          0.024691354708124207,
          0.17543859150507862,
          0,
          0.09195401879772777,
          0.09999999768888894,
          0.13157894361842115,
          0.019607839538639642,
          0.03539822537395317,
          0.16666666168209893,
          0.021505371390913384,
          0.16216215729364516,
          0.07058823035017335,
          0.11320754328230702,
          0.0967741889698233,
          0.0999999968000001,
          0.0799999950720003,
          0.04999999525312546,
          0.041666662033420655,
          0.13636363310950422,
          0.09523809187074841,
          0.061855665192900804,
          0.06611569749607304,
          0.1616161570451996,
          0.23333332942222226,
          0.2564102516765287,
          0.12765957046627446,
          0.09756097103807279,
          0.04878048295062511,
          0.05882352493512145,
          0.14634145972635346,
          0.03738317264389968,
          0.042553187904029274,
          0.18181817691115715,
          0.14814814383401934,
          0.03225806120187339,
          0.19565216959593584,
          0.13793103048751498,
          0.0563380231700064,
          0.08955223399420832,
          0.1818181772413224,
          0.19999999500408172,
          0.07547169341402665,
          0.05714285263673504,
          0.04444443974321038,
          0.15384614907810668,
          0.15254236789428344,
          0.09999999507200025,
          0.1911764656152683,
          0.2135922288811387,
          0.24615384158106512,
          0.17241378895957202,
          0.09677419042663901,
          0.06976743699296951,
          0.16666666202222233,
          0.20338982557885676,
          0.21874999523925792,
          0.1263157845673132,
          0.2295081928513841,
          0.11428570928616802,
          0.19999999510204094,
          0.13333332846439927,
          0.21428571091836737,
          0.30232557678474853,
          0.02898550339004462,
          0.10810810319941586,
          0.11538461074704161,
          0.025974022256704865,
          0.1199999953920002,
          0.077922073536853,
          0.09090908595041348,
          0.15929203044561063,
          0.06060605577135025,
          0.2891566222035129,
          0.0350877154201297,
          0.06956521266691904,
          0.029411760553633808,
          0.15789473252077574,
          0.09876542783112349,
          0.05263157472645463,
          0.1914893567587145,
          0.0666666616722226,
          0.028571423636735545,
          0.055555551543210166,
          0.26086956087376606,
          0.1538461490072322,
          0.07843136798154583,
          0.10526315457063723,
          0.13636363338842983,
          0.09374999625000015,
          0.39999999502812505,
          0.1123595456684764,
          0.13043477761814765,
          0.030303025321396598,
          0.1454545405884299,
          0.13043477784499072,
          0.07999999680000014,
          0.21874999500488293,
          0.23529411320261448,
          0.21052631082794718,
          0.1944444395408952,
          0.23255813584369936,
          0.20689654673008337,
          0,
          0.1929824511542014,
          0.16666666236979177,
          0.19753085920438967,
          0.15384614931952675,
          0.045454543990185996,
          0.06896551262187903,
          0.06557376550389718,
          0.1714285664448981,
          0.060606057630854145,
          0.15555555111111127,
          0.14492753146817913,
          0.1568627404229144,
          0.18181817685950424,
          0.21052631134502933,
          0.11320754268422945,
          0.10958903636704843,
          0.25454545002314055,
          0.17543859156663602,
          0.07017543618344114,
          0.09374999507812526,
          0.13559321609882233,
          0.15384614950368328,
          0.1886792405838378,
          0.03389830084458543,
          0.17094016649572658,
          0.025974022836903737,
          0,
          0.19565216931238197,
          0.043478258601134336,
          0.022727268163740582,
          0.07894736541551257,
          0.09999999511250024,
          0.08421052140941857,
          0.29629629173525385,
          0.08163264916284901,
          0.23684210084833804,
          0.2051282006837608,
          0.15999999528800013,
          0.08823528927335667,
          0.060606056732986684,
          0.14432989248591788,
          0.3030302984022039,
          0.14999999680000006,
          0.2545454497454546,
          0.1818181779935722,
          0.07792207373250148,
          0.22222221731111122
         ],
         "xaxis": "x3",
         "yaxis": "y3"
        },
        {
         "cumulative": {
          "enabled": false
         },
         "legendgroup": "rl_score",
         "name": "lead_sum (0.18)",
         "opacity": 0.5,
         "type": "histogram",
         "x": [
          0.13432835322677675,
          0.03448275376932292,
          0.12371133535976209,
          0.256410251965812,
          0.23880596596123863,
          0.1917808178194784,
          0.14736841607977857,
          0.2926829218560381,
          0.03846153559911264,
          0.19999999507200011,
          0.1458333285503474,
          0.21874999609863285,
          0.2758620642167064,
          0.1052631529085875,
          0.17910447270216096,
          0.18666666177422236,
          0.30434782155009454,
          0.30303029807162535,
          0.1526717512639125,
          0.2857142821492347,
          0.07142856827168381,
          0.12048192303091904,
          0.17142856822857147,
          0.22471909626562314,
          0.059701489062152134,
          0.2417582374157711,
          0.16129031908428726,
          0.22727272239927698,
          0.13333332833333353,
          0.15189872967793636,
          0.17021276101403365,
          0.12903225369406884,
          0.24561403090181602,
          0.04255318714350429,
          0.14634145848899482,
          0.3380281640230114,
          0.1720430067244769,
          0.15384614950368328,
          0.14999999501250016,
          0.17283950117969835,
          0.1632653024239901,
          0.18749999500122083,
          0.18181817737373748,
          0.17543859230532483,
          0.11111110709876558,
          0.2622950776780436,
          0.17021276133997296,
          0.046511624597079734,
          0.16393442124160187,
          0.1395348788182803,
          0.08333332859592041,
          0.10126581940714639,
          0.35714285216836733,
          0.26666666166666675,
          0.20338982562482055,
          0.17857142587372452,
          0.22429906146912396,
          0.1891891849050403,
          0.22641508938412258,
          0.11235954570382549,
          0.07547169441082255,
          0.2077922032383202,
          0.21374045301555863,
          0.20437955753636325,
          0.13533834094635103,
          0.06896551225789439,
          0.1538461493539429,
          0.31775700444405625,
          0.1034482740071344,
          0.25641025152859964,
          0.17391303962192828,
          0.2666666622987655,
          0.15999999580000013,
          0.16129031987513012,
          0.3333333284986227,
          0.14814814383401934,
          0.18518518024691372,
          0.06779660568802097,
          0.11999999564800015,
          0.12903225514568165,
          0.25352112176155533,
          0.13888888559027784,
          0.14285713862882662,
          0.05633802452687982,
          0.2318840533501366,
          0.1111111068672841,
          0.1842105214854572,
          0.1052631529085875,
          0.13636363162190102,
          0.06666666202222254,
          0.17391303913463568,
          0.15189872918442573,
          0.12499999781250003,
          0.2816901362824837,
          0.1391304312831759,
          0.09374999500000027,
          0.1499999960125001,
          0.1538461488757398,
          0.09999999745000007,
          0.1818181784066116,
          0.19999999515802477,
          0.2337662290099512,
          0.10256409930309017,
          0.2314049537763815,
          0.1917808169187466,
          0.21739129981096417,
          0.09677418859521357,
          0.1714285665306124,
          0.14925372637112957,
          0.2553191458397465,
          0.12345678516079886,
          0.1568627401922339,
          0.23999999507200007,
          0.23076922579881665,
          0.2181818134198348,
          0.25641025180802113,
          0.3999999950222222,
          0.14814814348422511,
          0.20618556203634833,
          0.15384614931952675,
          0.10666666275555571,
          0.15999999539200013,
          0.25925925475480116,
          0.2117647008830451,
          0.10909090589090918,
          0.19999999549387765,
          0.18604650667117376,
          0.16216215944485027,
          0.18604650689832355,
          0.08695651774837238,
          0.12307691810650907,
          0.17142856668979603,
          0.22222221742222234,
          0.1481481431672003,
          0.21621621127465315,
          0.11594202428901511,
          0.16393442235958086,
          0.18604650708491086,
          0.09230768740355055,
          0.16806722199279725,
          0.2352941126897819,
          0.0869565175047261,
          0.05333332859022265,
          0.18823528978269904,
          0.17977527595000642,
          0.3333333292013889,
          0.14814814328703718,
          0.23999999520000007,
          0.19354838439125913,
          0.15624999595703135,
          0.25925925436899866,
          0.1568627401922339,
          0.1194029819469816,
          0.17647058325259532,
          0.19354838439125913,
          0.2706766872044774,
          0.13513513043097167,
          0.3238095188317461,
          0.12195121537180266,
          0.03389830020109235,
          0.10989010515638228,
          0.28571428126984133,
          0.0810810761139521,
          0.09756097127900079,
          0.12658227383432163,
          0.2142857095153062,
          0.06451612416233128,
          0.26923076548076924,
          0.19999999555555567,
          0.16216215877282697,
          0.1269841220156213,
          0.117647053854287,
          0.2711864364378053,
          0.15730336596894345,
          0.15999999507200013,
          0.15384614884733747,
          0.18181817946280993,
          0.11111110736111124,
          0.1612903177679502,
          0.15384614927337292,
          0.14084506604245203,
          0.1714285669224491,
          0.1621621571658146,
          0.16393442171459296,
          0.1739130387145559,
          0.18181817914256201,
          0.23529411317041526,
          0.08108107894448509,
          0.2857142813807005,
          0.10958903799587173,
          0.1627906926852354,
          0.30769230275887577,
          0.09999999642222235,
          0.1860465076041104,
          0.2253521080341203,
          0.07999999510755584,
          0.2539682489795919,
          0.15555555085432113,
          0.1470588188062285,
          0.09523809187074841,
          0.41791044276230793,
          0.15999999528800013,
          0.19607842669742417,
          0.08771929374422922,
          0.14814814331961607,
          0.14492753153539187,
          0.11538461038461562,
          0.14925372659389638,
          0.19718309411624688,
          0.31999999520000005,
          0.12307692006627227,
          0.4545454495686984,
          0.20895522024504354,
          0.22784809628585176,
          0.2933333288035556,
          0.17647058339100358,
          0.09230768757396474,
          0.23076922650887582,
          0.18749999507812512,
          0.17391303865574473,
          0.12698412243890164,
          0.19672130731523796,
          0.27586206444708683,
          0.12244897519366944,
          0.15384614885355044,
          0.1754385940781779,
          0.1643835568549448,
          0.1379310296967897,
          0.23913042986767494,
          0.06666666308888908,
          0.166666662338889,
          0.10909090446281011,
          0.1785714236734695,
          0.21212120778236923,
          0.1682242942580139,
          0.13793102953626654,
          0.1866666616746668,
          0.16949152049411104,
          0.09374999641113295,
          0.17142856657551034,
          0.3414634098750744,
          0.14925372639786158,
          0.1521739082797733,
          0.1866666616746668,
          0.23880596515927835,
          0.1408450654314622,
          0.17021276095971044,
          0.10666666231466684,
          0.2941176426143791,
          0.33766233266992757,
          0.18181817719008275,
          0.2380952347278912,
          0.25531914444545045,
          0.17021276106835687,
          0.18918918537253473,
          0.27272726802112035,
          0.13793102996432832,
          0.04123711037942417,
          0.18556700537357862,
          0.09090908607438043,
          0.15999999539200013,
          0.1132075422570312,
          0.1379310296967897,
          0.13888888528163593,
          0.2142857093000639,
          0.30188678766820937,
          0.14893616586690822,
          0.14999999580000012,
          0.18421052134695307,
          0.1904761866099774,
          0.29411764346020763,
          0.17777777313333343,
          0.124999995395508,
          0.26315788979224386,
          0.14084506559016083,
          0.1904761854802722,
          0.27692307195266275,
          0.13333332835555575,
          0.15555555091111126,
          0.16949152074691196,
          0.1666666618055557,
          0.19999999509453137,
          0.12820512439184759,
          0.11538461098372799,
          0.26829267822427133,
          0.11111110652839526,
          0.09523809215419511,
          0.30188678749733006,
          0.18421052140235472,
          0.23529411292387553,
          0.17647058434256063,
          0.1538461491124262,
          0.16513760983082246,
          0.15094339316482738,
          0.18749999548828133,
          0.23684210048476464,
          0.18181817744311815,
          0.16666666222222234,
          0.1639344222090837,
          0.19354838211122685,
          0.24999999506172846,
          0.1473684160975071,
          0.14736841717451535,
          0.15999999539200013,
          0.12048192271156938,
          0.24561403043397972,
          0.3636363591140496,
          0.27999999512800006,
          0.1538461491124262,
          0.07999999731200008,
          0.07843136756632098,
          0.07792207306459806,
          0.1379310303388824,
          0.18181817693296617,
          0.12698412198538694,
          0.15999999680000007,
          0.34146340966091615,
          0.29268292241225463,
          0.04545454096074425,
          0.2033898276931916,
          0.18181817751658005,
          0.09677418929760684,
          0.13483145572528737,
          0.19047618547745035,
          0.12631578627368428,
          0.2272727237603306,
          0.1204819233212369,
          0.2666666620222223,
          0.11764705522491362,
          0.1666666617166668,
          0.3235294071972319,
          0.09195401798916661,
          0.17910447413677882,
          0.18556700537357862,
          0.16393442145659787,
          0.19999999647346942,
          0.0952380905215422,
          0.32835820404544447,
          0.26229507703305566,
          0.3119266005151082,
          0.10714285214923494,
          0.1428571384948981,
          0.17241378882283007,
          0.2752293528019527,
          0.1265822739048231,
          0.19178081703133806,
          0.23076922666420124,
          0.06153845688047372,
          0.17021276186283396,
          0.4137930991676576,
          0.05479451697504245,
          0.14545454049586792,
          0.11940298077077315,
          0.1860465074094106,
          0.2631578914127424,
          0.19753086060661487,
          0.12121211753902673,
          0.24778760590492607,
          0.061538457401183716,
          0.12371133529599339,
          0.22950819172265532,
          0.2272727222959712,
          0.18867924139551448,
          0.17241378839476829,
          0.10256409930309017,
          0.1199999950720002,
          0.08955223383381626,
          0.12765956979402462,
          0.13043477946124774,
          0.10810810442658889,
          0.1649484486980552,
          0.16071428071588023,
          0.18947367955678682,
          0.22641509012459957,
          0.3218390755132779,
          0.13043477855387536,
          0.18181817682479354,
          0.05128204636423453,
          0.14925372683448443,
          0.2439024353361095,
          0.14999999515312512,
          0.13333332963950628,
          0.08510637816206455,
          0.1463414585603809,
          0.15384615009615396,
          0.21739130003071838,
          0.20289854716236091,
          0.13698629637830753,
          0.09836065080354768,
          0.25454544996859507,
          0.18918918419284161,
          0.1886792405838378,
          0.07999999500800031,
          0.2380952332766441,
          0.12403100300462731,
          0.1983471024656787,
          0.14141413647586998,
          0.1999999950091838,
          0.21782177817076762,
          0.3098591505494942,
          0.1355932162367137,
          0.09677419042663901,
          0.0930232509464578,
          0.18461538013727818,
          0.17142856675918378,
          0.21874999523925792,
          0.12244897464389858,
          0.2499999955555556,
          0.15384614898200716,
          0.19444443958333346,
          0.15094339134033483,
          0.23529411404844294,
          0.3499999952531251,
          0.09090908694214893,
          0.18918918428049683,
          0.15094339162691361,
          0.2307692261316569,
          0.1568627405305653,
          0.11428571046530626,
          0.21739129943289237,
          0.13793102955558878,
          0.16393442129535088,
          0.30588234869480974,
          0.16393442252082782,
          0.17054263116399268,
          0.1904761861426053,
          0.14457830914211073,
          0.16438355714017652,
          0.12244897461057913,
          0.20618556210011701,
          0.10714285214923494,
          0.1249999950031252,
          0.055555551543210166,
          0.2571428527306123,
          0.18181817682277332,
          0.18518518074074086,
          0.19047618739229027,
          0.1428571397732427,
          0.11999999564800015,
          0.43373493481782555,
          0.17977527600555498,
          0.1276595694884565,
          0.16129031766389193,
          0.13953487874526788,
          0.24999999531250006,
          0.10256409875082198,
          0.21212120713957774,
          0.30188678809540764,
          0.140350872231456,
          0.2285714236367348,
          0.5454545404648761,
          0.23376622890875368,
          0.22535210775639763,
          0.35849056112495553,
          0.13333332888888905,
          0.18823528915155724,
          0.12195121509518161,
          0.07692307459319533,
          0.04761904261904814,
          0.2857142807206633,
          0.1690140795159692,
          0.05882352650519045,
          0.15053763003815482,
          0.24691357580246914,
          0.2272727223657026,
          0.20930232058409964,
          0.19354838284599382,
          0.12307691907218948,
          0.13559321535191057,
          0.25454545002314055,
          0.16666666167534736,
          0.07142856897959192,
          0.15624999507812518,
          0.18867924079743692,
          0.1649484494165162,
          0.16666666222222234,
          0.10344827158145084,
          0.19298245181902132,
          0.12195121652290311,
          0.1445783087066339,
          0.18749999526258695,
          0.0784313704728951,
          0.2539682489695138,
          0.10958903799587173,
          0.24561403090181602,
          0.08620689198127254,
          0.3076923030547338,
          0.23255813531638725,
          0.24691357597927152,
          0.2666666617555556,
          0.19607842669742417,
          0.12121211632690562,
          0.23214285260363526,
          0.1224489751249481,
          0.24242423779614333,
          0.13333332942222234,
          0.20338982583165768,
          0.1818181779935722,
          0.17948717532873118,
          0.21621621123082552
         ],
         "xaxis": "x3",
         "yaxis": "y3"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "bert_score",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "r1_score",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.6111111111111112,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "rl_score",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.22222222222222224,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "barmode": "overlay",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.7777777777777778,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.3888888888888889,
          0.6111111111111112
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          0.22222222222222224
         ]
        }
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "bench.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up temporary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'DATA_PATH' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-36ff7145d0ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCACHE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCACHE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "if os.path.exists(DATA_PATH):\n",
    "    shutil.rmtree(DATA_PATH, ignore_errors=True)\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    shutil.rmtree(CACHE_PATH, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('penv': virtualenv)",
   "language": "python",
   "name": "python37364bitpenvvirtualenv344c98667b1049f485b37814feec52b5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}